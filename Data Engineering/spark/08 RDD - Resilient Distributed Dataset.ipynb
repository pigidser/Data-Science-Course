{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/local/spark\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/home/pigidser/anaconda3/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python3\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"pyspark-shell\"\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"spark_test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание RDD\n",
    "# Давайте создадим простейший RDD - слова файла (как строки)\n",
    "\n",
    "wrds = spark.sparkContext.parallelize(\"this is spark and it is great\".split(),3) # 3 - number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Country,Region,Population,Area (sq. mi.),Pop. Density (per sq. mi.),Coastline (coast/area ratio),Net migration,Infant mortality (per 1000 births),GDP ($ per capita),Literacy (%),Phones (per 1000),Arable (%),Crops (%),Other (%),Climate,Birthrate,Deathrate,Agriculture,Industry,Service'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Из файла\n",
    "# Прочитаем файл и создадим RDD со строками файла\n",
    "\n",
    "lines = spark.sparkContext.textFile(\"countries_of_the_world.csv\")\n",
    "lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Трансформации\n",
    "# Начнем с фильтрации, давайте сделаем что-то со словами, ниже - разберем, что получилось\n",
    "# и почему RDD не эффективны\n",
    "\n",
    "def startsWithS(individual):\n",
    "    return individual.startswith(\"s\")\n",
    "\n",
    "wrds.filter(lambda word: startsWithS(word)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что произошло:\n",
    "\n",
    "- мы написали функцию на python (startsWithS)\n",
    "- она была spark-ом как-то переведена на java (executor-ы \"понимают\" только java)\n",
    "- на каждом узле кластера (где \"живут\" разделы нашего RDD) для каждого элемента раздела RDD была вызвана эта функция\n",
    "- на вход был подан \"переведенный\" в python строку (десериализация) элемент раздела RDD с этого узла\n",
    "- функция отработала, вернула True или False\n",
    "- в раздел нового RDD попала часть элементов старого (возможно, произошла еще одна сериализация)\n",
    "- все разделы собираются на драйвер (collect) и происходит еще одна десериализация (в типы python)\n",
    "\n",
    "Обратите внимание - результат collect() - список строк (чисто python объект), т.е. никакого преобразования в объекты spark не происходит. В функции startsWithS() мы работаем со строками. Spark делает за нас все преобразования, но это снижает эффективность, нужно иметь это в виду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it', 'is', 'and', 'this', 'spark', 'great']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dictinct()\n",
    "# У нас есть повторяющиеся слова - удалим их\n",
    "\n",
    "wrds.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 4),\n",
       " ('is', 2),\n",
       " ('spark', 5),\n",
       " ('and', 3),\n",
       " ('it', 2),\n",
       " ('is', 2),\n",
       " ('great', 5)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map()\n",
    "# Преобразуем наши слова в пары - слово, длина\n",
    "\n",
    "def getLen(individual):\n",
    "    return (individual,len(individual))\n",
    "\n",
    "wrds.map(getLen).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t', 'h', 'i', 's', 'i', 's']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatMap()\n",
    "# Разобъем слова по символам (т.е. увеличим количество разделов - каждый символ = раздел)\n",
    "# и соберем на driver первые 6 разделов\n",
    "\n",
    "def toChars(individual):\n",
    "    return list(individual)\n",
    "\n",
    "wrds.flatMap(toChars).take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark', 'great', 'this', 'and', 'is', 'it', 'is']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort()\n",
    "# Давайте упорядочим слова по их длине (по убыванию)\n",
    "\n",
    "wrds.sortBy(lambda word: len(word) * -1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Действия\n",
    "# мы уже видели collect()/take(), давайте посмотрим reduce(): оставим самое длинное слово из нашего набора\n",
    "\n",
    "def wordLengthReducer(leftWord, rightWord):\n",
    "    if len(leftWord) > len(rightWord):\n",
    "        return leftWord\n",
    "    else:\n",
    "        return rightWord\n",
    "\n",
    "wrds.reduce(wordLengthReducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count()\n",
    "\n",
    "wrds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saveAsTextFile()\n",
    "# Сохраним наши слова в виде \"файла\" - как обычно для Spark: будет создана директория,\n",
    "# в которую будет записан RDD, каждый раздел - отдельный файл. Количество разделов мы тоже выведем,\n",
    "# хотя это - 3 (мы именно так создавали RDD - см. выше).\n",
    "\n",
    "!rm -rf words.txt\n",
    "wrds.saveAsTextFile(\"words.txt\")\n",
    "wrds.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.0  as numbers\n",
      "5.0  as strings\n"
     ]
    }
   ],
   "source": [
    "numbs = spark.sparkContext.parallelize([1.0, 5.0, 43.0, 10.0])\n",
    "print(numbs.max(),\" as numbers\")\n",
    "print(numbs.max(key=str),\" as strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'spark', 'and', 'it', 'is', 'great']\n",
      "[['this', 'is'], ['spark', 'and'], ['it', 'is', 'great']]\n"
     ]
    }
   ],
   "source": [
    "# трансформация glom()\n",
    "# Собирает все элементы раздела в список.\n",
    "\n",
    "print(wrds.collect())\n",
    "print(wrds.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Кэширование\n",
    "# Как и с датафреймами - не сможем это эффективно использовать, но упомянуть стоит.\n",
    "# cache() - кэширует RDD (после выполнения любого следующего действия), после этого RDD уже не будет\n",
    "# каждый раз исполняться, значения будут браться из кэша.\n",
    "# После выполнения этого узла можно будет увидеть статус RDD здесь - localhost:4040/storage/\n",
    "\n",
    "wrds.cache()\n",
    "wrds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обратная операция - unpersist()\n",
    "# После ее выполнения кэш пропадет (см. localhost:4040/storage/)\n",
    "\n",
    "wrds.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'pete', {'id': 1, 'details': {'name': 'pete', 'phone': 123}}),\n",
       " (2, 'mike', {'id': 2, 'details': {'name': 'mike', 'phone': 999}})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Использование RDD для небольших XML файлов\n",
    "# Как один из вариантов практического использования RDD - работа с мелкими XML файлами\n",
    "# (помните, мы разбирали проблематику в модуле про работу с источниками?).\n",
    "# Можно считать XML (в преобразованном, конечно, виде) в RDD, а дальше с помощью map() \"вытащить\"\n",
    "# нужные поля в \"плоскую\" часть, оставив весь исходный XML в виде одного из полей.\n",
    "# Потом эту \"регулярную\" структуру сохранить в реляционной таблице, например.\n",
    "# Это будет не очень эффективно - см выше - но для обработки относительно небольшого потока XML может и хватить.\n",
    "\n",
    "dList = [ \n",
    "    { \"id\": 1, \"details\": { \"name\": \"pete\", \"phone\": 123 } },\n",
    "    { \"id\": 2, \"details\": { \"name\": \"mike\", \"phone\": 999 } },\n",
    "]\n",
    "dictRdd = spark.sparkContext.parallelize(dList)\n",
    "\n",
    "def mkRecord(el):\n",
    "    return ( el[\"id\"], el[\"details\"][\"name\"], el)\n",
    "\n",
    "dictRdd.map(mkRecord).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Пример вычисления числа Пи\n",
    "Давайте, наконец, разберем - что же происходило в том примере, который мы использовали для вычисления числа Пи (в начале - когда тестировали работоспособность Spark), код приведен ниже. Теперь мы все знаем и можем объяснить - собственно, один из \"смыслов\" изучения RDD:\n",
    "\n",
    "- создается простейщий RDD (один раздел, список чисел нужного диапазона)\n",
    "- функция filter() позволяет нам выполнить код на питоне (вообще говоря - любой), в этом примере параметр (элемент RDD, для которого происходит вызов) не используется\n",
    "- функция inside() написана на python, использует его инструментарий\n",
    "- возвращает true или false, в зависимости от того, попали ли случайные числа в круг диаметром 2 или нет\n",
    "- фильтруя мы оставляем в RDD только \"попавшие\" в круг его элементы\n",
    "- count() посчитает число попавших в круг элементов\n",
    "- далее - простая математика (которая геометрия)\n",
    "\n",
    "Все просто! (\"как хорошо уметь читать...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141632\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_samples = 1000000\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = spark.sparkContext.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
